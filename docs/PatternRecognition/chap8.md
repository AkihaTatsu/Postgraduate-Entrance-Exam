# 非参数学习与集成学习

## 近邻法
### 最近邻法
已知样本集$S_N = \{(\bm{x}_1, \theta_1), (\bm{x}_2, \theta_2), ..., (\bm{x}_N, \theta_N)\}$，其中$\bm{x}_i$为样本$i$的特征向量，$\theta_i$为它对应的类别。假设有$c$个类，即$\theta_i \in \{1, 2, ..., c\}$。定义两个样本间距离度量为$\delta(\bm{x}_i, \bm{x}_j)$，例如欧氏距离$\delta(\bm{x}_i, \bm{x}_j) = \parallel \bm{x}_i - \bm{x}_j \parallel$。
对未知样本$\bm{x}$，求$S_N$中与之距离最近的样本，设为$\bm{x}'$（对应类别为$\theta'$），即$$\delta(\bm{x}, \bm{x}') = \min_{j = 1, 2, ..., N} \delta(\bm{x}, \bm{x}_j)$$则将$\bm{x}$决策为$\theta'$类。
如果写成判别函数形式，$\omega_i$类的判别函数可以写作
$$g_i(\bm{x}) = \min_{\bm{x}_j \in \omega_i} \delta(\bm{x}, \bm{x}_j),\quad i = 1, 2, ..., c$$

决策规则为各类的判别函数比较大小，即
> 若$g_k(\bm{x}) = \min\limits_{i = 1, 2, ..., c} g_i(\bm{x})$，则$\bm{x} \in \omega_k$。

在已知样本数足够多时，最近邻决策可以取得较好效果。对于其错误率，理论上有如下结果：
设$N$个样本下最近邻法的平均错误率为$P_N(e)$，样本$\bm{x}$的最近邻为$\bm{x}'$，平均错误率可以写成$$P_N(e) = \int\int P_N(e | \bm{x}, \bm{x}') p(\bm{x}' | \bm{x}) \,d\bm{x}'p(\bm{x}) \,d\bm{x}$$

定义最近邻法的渐进错误率$P$为：当$N \to \infty$时$P_N(e)$的极限，即$P = \lim\limits_{N\to\infty} P_N(e)$，则可以证明存在关系
$$P^* \leq P \leq P^* \left(2 - \dfrac{c}{c - 1}P^*\right)$$其中$P^*$为贝叶斯错误率（即理论上最优的错误率），$c$为类别数。由上可知，最近邻法的渐进错误率最坏不超过两倍的贝叶斯错误率，最好则有可能接近贝叶斯错误率。

### $k$-近邻法
简单描述：考虑最近$k$个样本，最多的类别作为新样本的类别。

设有$N$个已知样本分属于$c$个类$\omega_i,\ i = 1, 2, ..., c$，考查新样本$\bm{x}$在这些样本中的前$k$个近邻，设其中有$k_i$个属于$\omega_i$类，则$\omega_i$类的判别函数就是$$g_i(\bm{x}) = k_i,\quad 1, 2, ..., c$$

决策规则为
> 若$g_k(\bm{x}) = \max\limits_{i = 1, 2, ..., c} g_i(\bm{x})$，则$\bm{x} \in \omega_k$。

通常二分类时设置$k$为奇数，避免得票相等情况。
可以引入距离远近的加权、在多分类时增加投票相同的处理等启发式方法。

### 近邻法的快速算法
用$\mathscr{X} = \{\bm{x}_1, \bm{x}_2, ..., \bm{x}_N\}$表示样本集，我们的目标是在$\mathscr{X}$中寻找未知样本$\bm{x}$的$k$个近邻。为简单起见，首先讨论$k = 1$的情况。

1. **第一阶段：样本集$\mathscr{X}$的分级分解**
   首先将$\mathscr{X}$分为$l$个子集，每个子集再分成$l$个子集，依此类推就能得到一个多层树结构，每个节点对应若干个样本。考虑节点$p$，用下列参数表示$p$节点对应样本子集：
   + $\mathscr{X}_p$：节点$p$对应的样本子集；
   + $N_p$：$\mathscr{X}_p$中的样本数；
   + $M_p$：样本子集$\mathscr{X}_p$中的样本均值；
   + $r_p = \max\limits_{\bm{x}_i \in \mathscr{X}_p} D(\bm{x}_i, M_p)$：从$M_p$到$\bm{x}_i \in \mathscr{X}_p$的最大距离，即该样本子集中离均值点最远样本和均值点间的距离；
  
   划分样本集的方法很多，如聚类就是一种有效的划分方法。
2. **第二阶段：搜索**
   先给出两个规则。
   + 规则一：如果存在$$D(\bm{x}, M_p) > B + r_p$$则$\bm{x}_i \in \mathscr{X}_p$不可能是$\bm{x}$的最近邻；其中$B$为算法执行过程中，已考察样本到$\bm{x}$的最近距离。一般$B$初始值可设$B = +\infty$。
   + 规则二：如果存在$$D(\bm{x}, M_p) > B + D(\bm{x}_i, M_p)$$其中$\bm{x}_i \in \mathscr{X}_p$，则$\bm{x}_i \in \mathscr{X}_p$不可能是$\bm{x}$的最近邻。

   从而可以设计出如下树搜索算法（待搜索的为样本$\bm{x}$）：
   1. **初始化：** 置$B = +\infty, L = 1, p = 0$。其中$L$为当前所在树的层数，$p$为当前节点。
   2. **子节点入队：** 将当前节点的所有直接后继节点放入一个目录表中，对这些节点计算$D(\bm{x}, M_p)$。
   3. **去掉过远节点（样本集）：** 对步骤2中的每个节点$p$，根据规则1，如果有$D(\bm{x}, M_p) > B + r_p$，则从目录表中去掉$p$。
   4. **检查队列状况、进行可能的层级变动：** 如果步骤3的目录表中已经没有节点，则后退到前一个层级，即置$L = L - 1$。如果此时$L = 0$则停止；反之，重复步骤3。如果目录表中有一个以上的节点存在，则转步骤5。
   5. **从最近节点（样本集）开始考察：** 在目录表中选择最近节点$p'$使得$D(\bm{x}, M_p)$最小化，并记$p'$为当前执行节点，从目录表中去掉$p'$。如果当前层级$L$为最低层级（叶子节点），则转步骤6；反之，置$L = L + 1$，转步骤2。
   6. **考察叶子节点的每个样本：** 对现在执行节点$p'$中的每个$\bm{x}$，利用规则2进行如下检验：如果$$D(\bm{x}, M_p) > B + D(\bm{x}_i, M_p)$$则$\bm{x}_i$不是$\bm{x}$的最近邻，从而不计算$D(\bm{x}, \bm{x}_i)$；否则，计算$D(\bm{x}, \bm{x}_i)$，如果$$D(\bm{x}, \bm{x}_i) < B$$则置$B = D(\bm{x}, \bm{x}_i)$和最近邻下标$NN = i$。在当前执行节点中所有$\bm{x}_i$被检验之后，转步骤3。

   最终得到的结果为最近邻$\bm{x}_{NN}$和对应距离$B = D(\bm{x}, \bm{x}_{NN})$。
   对于$k$-近邻法，首先将$B$的定义改为 **$\bm{x}$到第$k$个近邻的距离**，然后当步骤6中没计算一个距离之后，就与当前执行近邻表中的$k$个近邻距离比较，如果新计算的距离小于其中任何一个时，则从近邻表中去掉最大的一个，插入新距离，记录对应下标，并更新$B$为近邻表中最大距离。

### 剪辑近邻法
有些时候，两类数据分布会存在一定程度的重叠，如果能将样本混杂的交界区域去掉，近邻法的决策面会更接近最优分类面。
一种代表性做法是，将已知样本集划分为考试集$\mathscr{X}^{NT}$和训练集$\mathscr{X}^{NR}$两部分，用训练集$\mathscr{X}^{NR}$中的样本对考试集$\mathscr{X}^{NT}$中的样本进行近邻法分类，从考试集$\mathscr{X}^{NT}$中去掉错误分类的样本，剩余样本构成剪辑样本集$\mathscr{X}^{NTE}$对未来样本进行近邻法分类。

为消除考试集、训练集中划分偶然性影响，引入**多重剪辑方法**（MULTIEDIT）：
1. **划分：** 把样本随机划分为$s$个子集，$\mathscr{X}_1, \mathscr{X}_2, ..., \mathscr{X}_s,\ s \geq 3$。
2. **分类：** 用$\mathscr{X}_{(i + 1)\text{mod}(s)}$对$\mathscr{X}_i$中的样本分类，$i = 1, 2, ..., s$。例如，对$s = 3$，用$\mathscr{X}_2$对$\mathscr{X}_1$分类，用$\mathscr{X}_3$对$\mathscr{X}_2$分类，用$\mathscr{X}_1$对$\mathscr{X}_3$分类。
3. **剪辑：** 从各个子集中去掉在步骤2里错分的样本。
4. **混合：** 将剩下的样板合在一起，形成新的样本集$\mathscr{X}^{NE}$。
5. **迭代：** 用$\mathscr{X}^{NE}$替代原样本集，转步骤1。如果在最近$m$次迭代中都没有样本被剪掉，则终止迭代，用最后的$\mathscr{X}^{NE}$作为剪辑后的样本集。

### 压缩近邻法
为了去掉远离分类边界的样本，简化决策过程中的计算，采用**压缩近邻法**（CONDENSE）：

1. 将样本集$\mathscr{X}^N$分为$\mathscr{X}_S$和$\mathscr{X}_G$两个活动的子集，前者为**储存集**，后者为**备选集**。
2. 算法开始时，$\mathscr{X}_S$只有一个样本，其余样本均在$\mathscr{X}_G$中。
3. 考察$\mathscr{X}_G$的每一个样本$\bm{x}$，如果用$\mathscr{X}_S$中的样本能够对其正确分类，则该样本留在$\mathscr{X}_G$，否则移动到$\mathscr{X}_S$中。
4. 最后用$\mathscr{X}_S$中的样本作为代表样本进行近邻法分类。

该方法可以与剪辑近邻法配合使用，如先剪辑再压缩。
值得一提的是，这种思想和支持向量机方法存在一定类似之处，都是从样本集中选取具有代表性的样本，提高分类面的推广能力。

## 决策树与随机森林
### 决策树
#### ID3方法（交互式二分法）
如果一个事件包含$k$种可能的结果，每种结果对应的概率$P_i,\ i = 1, 2, ..., k$，则我们对此事件的结果进行观察后得到的信息量可以用如下定义的熵来度量：
$$I = -\sum_{i = 1}^k P_i \log_2 P_i$$

对某个节点上的样本，我们把这个度量称为**熵不纯度**，其反映了该节点上特征对样本分类的**不纯度**。
如果特征把$N$个样本划分成$m$组，每组$N_m$个样本，则熵不纯度减少量为
$$\Delta I(N) = I(N) - \sum_{i = 1}^m \dfrac{N_m}{N} I(N_m)$$

ID3算法流程：
1. 计算当前节点包含的所有熵不纯度；
2. 比较采用不同特征进行分枝将会得到的信息增益，即不确定性减少量；
3. 选取具有最大信息增益的特征，赋予当前节点；
4. 如果后继结点只包含一类样本，则停止该枝生长，该节点成为叶节点；否则重复以上步骤。

#### C4.5算法
用信息增益率代替信息增益：$$\Delta I_R(N) = \dfrac{\Delta I(N)}{I(N)}$$

同时对连续数值添加了单独的处理：
如果数值特征$x$在训练样本上共包含了$n$个取值，把它们按照从小到大的顺序排序，得到$v_i,\ i = 1, 2, ..., n$；用二分法选择阈值把这组数值划分，共有$n - 1$种可能的划分方案；对每一种方案计算信息增益率，选择增益率最大的方案把该连续特征离散化为二值特征，再与其他非数值特征一起构建决策树。离散化为多值同理可得。

#### CART算法
基本思想和前二者相同，但每个节点都只进行二分，最终得到二叉树。
该算法既可以用于分类问题，也可以构造回归树对连续变量进行回归。

### 过学习与决策树的剪枝
#### 先剪枝
即控制决策树的生长，在决策树生长过程中决定某节点是否需要继续分枝还是直接作为叶节点。
常用判断停止方法：
+ **数据划分法：** 将数据分成训练样本和测试样本，基于训练样本进行成长，直到在测试样本上的分类错误率达到最小。常用多次交叉验证法来充分利用数据信息。
+ **阈值法：** 预设一个信息增益阈值，当信息增益小于设定阈值时停止生长。
+ **信息增益的统计显著性分析：** 对已有节点获得的所有信息增益统计其分布，如果继续生长得到的信息增益与该分布相比不显著，则停止树的生长。

#### 后剪枝
在决策树得到充分生长以后再对其进行修剪。
常用简直规则：
+ **减少分类错误修剪法：** 通过独立的剪枝集估计剪枝前后分类错误率的改变，判断是否需要合并分枝。
+ **最小代价与复杂性的折中：** 对合并分枝后产生的错误率增加与复杂性减少进行折中考虑。
+ **最小描述长度（MDL）准则：** 核心思想就是“最简单的树就是最好的树”。首先对决策数进行编码，再通过剪枝得到编码最短的决策树。

### 随机森林
基本步骤：
1. 对样本数据进行**自举重采样**，得到多个样本集。所谓自举重采样，就是每次从原来的$N$个训练样本中有放回地随机抽取$N$个样本（可能重复）。
2. 用每个重采样样本集作为训练样本构造一个决策树。在构造决策树的过程中，每次从所有候选特征中随机抽取$m$个特征，作为当前节点下决策的备选特征，从这些特征中选择最好地划分训练样本的特征。
3. 得到所需数目的决策树后，随机森林方法对这些树的输出进行投票，以得票最多的类作为随机森林的决策。

## Boosting集成学习（AdaBoost）
Boosting方法：通过一个迭代过程，对多个弱分类器的输入输出进行加权处理。
### AdaBoost
设给定$N$个训练样本$\{\bm{x}_1, \bm{x}_2, ..., \bm{x}_N\}$，用$f_m(\bm{x}) \in \{-1, 1\},\ m = 1, 2, ..., M$表示$M$个弱分类器在样本$\bm{x}$上的输出，通过AdaBoost算法构造这$M$个分类器并进行决策的具体过程如下：
1. **初始化权重：** 初始化训练样本$\{\bm{x}_1, \bm{x}_2, ..., \bm{x}_N\}$的权重$w_i = \dfrac{1}{N},\ i = 1, 2, ..., N$。
2. **权重调整：** 对$m = 1 \to M$，重复以下过程：
   1. 利用$\{w_i\}$加权后的训练样本构造分类器$f_m(\bm{x}) \in \{-1, 1\}$。每个弱分类器的算法可以不尽相同，如线性分类器、决策树等。
   2. 计算样本用$\{w_i\}$加权后的分类错误率$e_m$，并令$c_m = \log\dfrac{1 - e_m}{e_m}$。
   3. 令$$w_i = w_i \exp(c_m \cdot \mathbb{I}(y_i \neq f_m(\bm{x}_i))) = w_i \exp(\log\dfrac{1 - e_m}{e_m} \cdot \mathbb{I}(y_i \neq f_m(\bm{x}_i)))$$其中$\mathbb{I}(\cdot)$为指示函数，在括号中条件成立时取$1$，反之取$0$。
   4. 进行归一化，使得$\sum\limits_{i = 1}^N w_i = 1$
3. **输出：** 对于待分类样本$\bm{x}$，分类器的输出为$$\text{sgn}\left(\sum\limits_{m = 1}^M c_m f_m(\bm{x})\right) = \text{sgn}\left(\sum\limits_{m = 1}^M \log\dfrac{1 - e_m}{e_m} f_m(\bm{x})\right)$$

在第2.1步中，利用加权后的训练样本构造分类器，是指对分类器算法目标函数中的各个样本所对应的项进行加权，因此需要根据具体采用的分类器类型进行具体分析。例如，对于最小平方误差判别，加权后的最小平方误差准则函数为
$$\sum_{i = 1}^N w_i(\bm{\alpha}^T \bm{x}_i - y_i)^2$$

而对于决策树或者一些其他方法，则可以根据每个样本的权值调整重采样的概率，按照这个概率对样本进行重采样，用重采样得到的样本集构造新的弱分类器。
