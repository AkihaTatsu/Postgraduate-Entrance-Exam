# 统计学习理论概要

## 机器学习问题的提法
### 机器学习问题的函数表示
已知变量$y$和输入$\bm{x}$之间存在一定的未知依赖关系，即存在一个未知的联合概率密度函数$F(\bm{x}, y)$，机器学习就是根据$l$个独立同分布观察样本$$(\bm{x}_1, y_1), (\bm{x}_2, y_2), ..., (\bm{x}_l, y_l)$$在一个函数集$\{f(\bm{x}, \alpha),\ \alpha \in \varLambda\}$中求最优函数$f(\bm{x}, a_0)$，使得它给出的期望风险
$$R(\alpha) = \int L(y, f(\bm{x}, a_0))\,dF(\bm{x}, y)$$

最小。其中，$\{f(\bm{x}, \alpha),\ \alpha \in \varLambda\}$为**候选函数集**，$\alpha \in \varLambda$为**函数的广义参数**，$L(y, f(\bm{x}, a_0))$表示**用$f(\bm{x}, \alpha)$对$y$进行预测而造成的损失**。$R(\alpha)$为函数$f(\bm{x}, \alpha)$的函数，故称作**期望风险泛函**。不同类型的学习问题有不同形式的损失函数。

三类基本的机器学习问题：
+ **模式识别**（这里只讨论**监督模式识别**）**：** 系统输出为类别标号，对二分类问题，$y = \{0, 1\}$或$y = \{-1, 1\}$是二值函数。损失函数定义为$$L(y, f(\bm{x}, a_0)) = \begin{cases}
    0 & y = f(\bm{x}, \alpha) \\
    1 & y \neq f(\bm{x}, \alpha) \\
\end{cases}$$期望风险为平均错误率。
+ **函数拟合：** 损失函数定义为$$L(y, f(\bm{x}, a_0)) = (y - f(\bm{x}, \alpha))^2$$
+ **概率密度估计：** 由于不存在任何预测输出，学习的目的是使得到的概率密度函数$p(x, \alpha)$能够最好地描述训练样本集，从而学习的损失函数为模型的负对数似然函数，即$$L(p(\bm{x}, a_0)) = -\log p(\bm{x}, a_0)$$

### 经验风险最小化原则和其存在的问题
事实上，直接最小化$R(\alpha)$需要对服从联合概率密度$F(\bm{x}, y)$的所有可能样本及其输出值求期望，这在$F(\bm{x}, y)$未知的情况下无法进行。

由大数定理，$(\bm{x}_1, y_1), (\bm{x}_2, y_2), ..., (\bm{x}_l, y_l)$为$F(\bm{x}, y)$中的采样，故考虑用算术平均代替数学期望，从而得到**经验风险**$$R_{\text{emp}}(\alpha) = \dfrac{1}{l} \sum_{i = 1}^l L(y, f(\bm{x}, a_0))$$

这种在训练样本上最小化错误或风险的策略成为**经验风险最小化原则**，简称**ERM原则**。

历史上大部分机器学习方法都是用最小化经验风险来替代最小化期望风险的目标，如：
+ **感知器：** 学习目标：$\min J_P(\alpha) = \sum\limits_{y_j \in Y^k} (-\alpha^T y_j)$
+ **线性回归：** 学习目标：$\min E(\bm{w}) = \dfrac{1}{N} \sum\limits_{j = 1}^l (\bm{w}^T \bm{x}_j - y_j)^2$
+ **Logistic回归：** 学习目标：$\min E(\bm{w}) = \dfrac{1}{N} \sum\limits_{j = 1}^l \ln(1 + e^{-y_j \bm{w}^T \bm{x}_j})$
+ **多层感知器神经网络：** 学习目标：$\min E(\bm{w}) = \dfrac{1}{2} \sum\limits_{j = 1}^l (y - \hat{y}_{\text{MLP}})^2$

然而，这一原则存在以下问题：
1. 概率论中的大数定律只说明了随机变量的均值在样本倾向于无穷大时会收敛于其期望，但这个定律对于泛函是否仍然成立？
2. 所谓用$R_{\text{emp}}(\alpha)$近似$R(\alpha)$或当样本趋向无穷多时$R_{\text{emp}}(\alpha)$收敛于$R(\alpha)$，应该用什么来作为两个函数接近程度的度量？
3. 即使假设随着样本量增加，最小化经验风险的解同样能最小化期望风险，实际问题中，需要多少样本才能达到接近无穷多的效果？如果样本远非无穷多且有限，经验风险最小化是否还可行？得到的解是否有推广能力？

研究者在回答这些问题的基础上，发展出了**统计学习理论**：
1. 经验风险最小化学习过程一致的概念及充分必要条件。它回答了在样本趋向于无穷多的情况下，什么样的函数集可以采样经验风险最小化原则进行学习。
2. 采样经验风险最小化的学习过程，随着样本数母的增加收敛速度有多快。
3. 如何控制学习过程中的收敛速度，有限样本下机器学习的结构风险最小化原则。
4. 在结构风险最小化原则下设计机器学习算法，包括支持向量机推广能力的理论依据。

## 学习过程的一致性
回顾问题：
已知变量$y$和输入$\bm{x}$服从的联合概率密度函数$F(\bm{x}, y)$，机器学习就是在函数集$\{f(\bm{x}, \alpha),\ \alpha \in \varLambda\}$中求最优函数$f(\bm{x}, a_0)$，使得它给出的期望风险
$$R(\alpha) = \int L(y, f(\bm{x}, a_0))\,dF(\bm{x}, y)$$最小。
对$n$个样本$(\bm{x}_1, y_1), (\bm{x}_2, y_2), ..., (\bm{x}_l, y_l)$，我们定义经验风险为$$R_{\text{emp}}(\alpha) = \dfrac{1}{l} \sum_{i = 1}^l L(y, f(\bm{x}, a_0))$$

所谓**学习过程的一致性**，指的是对于函数集$\{f(\bm{x}, \alpha),\ \alpha \in \varLambda\}$和联合密度函数$F(\bm{x}, y)$，以下两个序列在样本趋向于无穷多时，依概率收敛到同一个极限：
$$
\lim_{l \to \infty} R(\alpha_l) \overset{P}{\rightarrow} \inf_{\alpha \in \varLambda} R(\alpha) \overset{P}{\leftarrow} \lim_{l \to \infty} R_{\text{emp}}(\alpha_l)
$$

如果函数集中包含某个函数，其期望风险和经验风险均取到最小，则上述条件可以满足，反之从函数集中去掉该函数后，上述条件就不一定能够得到满足。这种情况下的一致性是没有意义的，因为其一致性只是因为函数集包含了这样的特殊函数，而不能说明学习算法具有根据训练数据从函数集中选择函数的能力。
为了保证一致性是机器学习方法的真实性质，而不是由于函数集中个别函数导致的，统计学习理论提出了**非平凡一致性**，即要求上式对于函数集的所有子集均成立。下文所有一致性均指非平凡一致性。

**学习理论关键定理：** 
对于有界的损失函数，经验风险最小化学习一致的充分必要条件为，经验风险在如下意义上一致地收敛于真实风险：
$$\lim_{l \to \infty} P(\sup (R(\alpha_l) - R_{\text{emp}}(\alpha_l)) > \varepsilon) = 0,\quad \forall \varepsilon > 0$$

从上式可以看出，如果一个机器学习方法在最坏情况下仍能表现良好，则我们对它的推广能力有信心（但这一方法有些偏悲观，被部分人认为过于保守）。

## 函数集的容量与VC维
一个**指示函数集的容量**也就是用函数集中的函数对各种样本实现分类的能力，容量（capacity）与能力为同一个词。统计学习理论用**函数集在一组样本集上可能实现的分类方案数**来**度量函数集的容量**，把这个容量的对数在符合同一分布的样本集上的期望称作**函数集的熵**，而把容量对数在所有可能样本集上的上界定义为函数集的**生长函数**，其是关于样本数目$l$的函数，记为$G(l)$，反映了函数集在所有可能的$l$个样本上最大能力或容量。显然，$G(l) \leq l \ln 2$。

**定理：** 函数集学习过程一致收敛的充分必要条件是，对任意的样本分布，都有$$\lim_{l \to \infty} \dfrac{G(l)}{l} = 0$$

此时学习过程收敛速度一定是快的，也就是满足$$P(R(\alpha^* | l) - R(\alpha_0) > \varepsilon) \leq e^{-c\varepsilon^2 t}$$其中$c>0$为常数。

### VC维
一个函数集的生长函数，如果不是一直满足$G(l) = l \ln 2$，则一定在样本数增加到某个值$h$后满足下面的界：
$$G(l) \leq h(\ln \dfrac{l}{h} + 1),\quad l > h$$

这个特殊的样本数$h$被定义为函数集的**VC维**。如果这个值为无穷大，即无论样本数有多大都有$G(l) \leq l \ln 2$，则称函数集的VC维为无穷大。

直观理解，函数集的VC维度量了当样本数目增加到多少之后，函数集的能力就不会再跟随样本数等比例增长。从而，**VC维有限是学习过程一致性的充分必要条件**。

**直观定义：** 假设有$h$个样本的样本集能被一个函数集中的函数按照所有可能的$2^h$种形式分为两类，则称函数集能把样本数为$h$的样本集打散。指示函数集的VC维，就是用这个函数集中的函数所能打散的最大样本集的样本数目。
也就是说，如果存在某个包含$h$个样本的样本集能够被函数集打散，而不存在任何有$h + 1$个样本的样本集能被函数集打散，则函数集的VC维就是$h$。如果对于任意的样本数，总能找到一个样本集能够被这个函数集打散，则函数集的VC维就是无穷大。

根据上述定义，常见$d$维空间中的线性分类器
$$f(\bm{x}, \bm{w}) = \text{sgn}(\sum_{i = 1}^d w_i x_i + w_0)$$的VC维为$d + 1$。
$d$维空间中的线性回归函数
$$f(\bm{x}, \bm{w}) = \sum_{i = 1}^d w_i x_i + w_0$$的VC维也是$d + 1$。

然而，正弦函数和其派生的分类器集合
$$f(x, \alpha) = \sin(\alpha x), \qquad f(x, \alpha) = \text{sgn}(\sin(\alpha x))$$的VC维为无穷大。
可见函数集的VC维并不简单地与函数中的自由参数个数有关，而是与函数本身的复杂程度有关。

## 推广能力的界与结构风险最小化原则
**定理：** 对于二分类问题，对指数函数集中的所有函数（包括使经验风险最小化的函数），经验风险和实际风险之间至少以概率$1 - \eta$满足如下关系：
$$R(\alpha) \leq R_{\text{emp}}(\alpha) + \dfrac{1}{2}\sqrt{E}$$

其中，当函数集包含无穷多个元素时，有
$$E = 4\cdot \dfrac{h(\ln \dfrac{2l}{h} + 1) - \ln \dfrac{\eta}{4}}{l}$$

而当函数集包含有限个（$N$个）元素时，有
$$E = 4\cdot \dfrac{\ln N - \ln \eta}{l}$$

通常，我们面对的函数都包含$50$多个可能的函数，从而该上界可以写成
$$R(\alpha) \leq R_{\text{emp}}(\alpha) + \sqrt{\dfrac{h(\ln \dfrac{2l}{h} + 1) - \ln \dfrac{\eta}{4}}{l}} = R_{\text{emp}}(\alpha) + \varPhi(\dfrac{h}{l})$$

在有限样本下，期望风险可能大于经验风险，超出部分的最大上界$\varPhi(\dfrac{h}{l})$被称为**置信范围**（confidence interval，与统计学置信区间并非一个概念，部分人也译作“VC置信”）。

当$\dfrac{h}{l}$较小时（如小于20），置信范围$\varPhi(\dfrac{h}{l})$较大，用经验风险最小化取得的最优解具有较大的期望风险，可推广性差；反之如果样本数较多，$\dfrac{h}{l}$较大，则置信范围较小，经验风险最小化得到的解接近于实际最优解。

由于VC维是函数集的性质而非单个函数的性质，因此$R(\alpha) \leq R_{\text{emp}}(\alpha) + \varPhi(\dfrac{h}{l})$右边的两项无法直接通过优化算法来最小化。
一种一般性的解决策略为：首先将函数集$S = \{f(x, \alpha),\ \alpha \in \varLambda\}$分解为一个函数子集序列（或子集结构）$$S_1 \subset S_2 \subset ... \subset S_k \subset ... \subset S$$使得各个子集能够按照置信范围$\varPhi$的大小（即VC维的大小）从小到大排列：$$h_1 \leq h_2 \leq ... \leq h_k \leq ...$$

完成了这样的函数子集结构划分后，学习的目标就变成在函数集中同时进行子集的选择和子集中最优函数的选择。选择最小经验风险和置信范围之和最小的子集，就可以达到期望风险的最小，这个子集中是的经验风险最小的函数就是要求的最优函数。这种思想称为**结构风险最小化原则**，简称**SRM原则**。
一个合理的函数子集结构应满足：
+ 每个子集的VC维有限且满足上方的关系；
+ 每个子集中函数对应的损失函数或是有界的非负函数，或是无界但幂次有限的函数。

这样的函数子集被称为**容许结构**。

统计学习理论的一个基本结论是，在有限样本下，设计和训练学习机器不应该采用经验风险最小化原则，而应该采用结构风险最小化原则。

## 支持向量机的理论分析
**$\varDelta$间隔超平面：** 对$d$维空间中权值归一化的超平面$$(w^* \cdot x) - b = 0,\quad \parallel w^* \parallel = 1$$

如果它把样本用以下的形式分开
$$y = \begin{cases}
    1 & (w^* \cdot x) - b \geq \varDelta \\
    -1 & (w^* \cdot x) - b \leq -\varDelta \\
\end{cases}$$

则称该超平面为$\varDelta$间隔超平面。具有间隔$\varDelta$的超平面构成函数子集，它的VC维有下面的界：
**定理：** 设样本集中在空间中属于一个半径为$R$的超球范围内，$\varDelta$间隔超平面的VC维$h$满足$$h \leq \min\left(\left\lfloor \dfrac{R^2}{\varDelta^2} \right\rfloor, d\right) + 1$$

前面提到，$d$维空间中不加约束的线性函数集的VC维是$d + 1$，而对于间隔为$\varDelta$的线性函数子集来说，如果这个间隔足够大，则函数子集的VC维将主要由间隔决定，有可能小于甚至远小于空间维数。

把这个定理转化为支持向量机中采用的规范化超平面的形式，结论为：如果$d$维空间中规范化超平面权值的模为$\parallel w \parallel \leq A$，则函数子集的VC维满足
$$h \leq \min (\lfloor R^2 A^2 \rfloor, d) + 1$$

其中$R$为空间中包含全部训练样本的最小超球的半径。
所以，支持向量机中最大化分类间隔，就是通过最小化$A$来实现最小化函数子集VC维的上界。在高维空间（尤其是经过核函数变换）的高维空间中，空间维数很大甚至是无穷大，但通过控制分类间隔，可以有效控制函数子集的VC维，从而保证在函数子集中求得经验风险最小的解具有好的推广能力。

**定理：** 如果包含$l$个样本的训练集被最大间隔超平面分开，那么超平面在未来独立测试集上测试错误率的期望有如下的界
$$E[P_{\text{error}}] \leq E\left[\min\left(\dfrac{m}{l}, \dfrac{\lfloor R^2 \parallel w \parallel^2 \rfloor}{l}, \dfrac{d}{l}\right)\right]$$

其中$m$为支持向量个数，$R$为包含数据的超球半径，$\dfrac{2}{\parallel w \parallel}$为分类间隔，$d$为空间的维数。

在高维空间小样本问题上，由于支持向量机对函数子集进行了控制，期望的测试错误率上界不再由原空间的维数决定，可以大大降低；同时，支持向量数目在全部样本中占的小比例也体现了学习后的推广能力。比例越小则期望的测试错误率上界越小。这些理论保证了高维小样本上支持向量机的出色表现，以及引入核函数进行等效升维后良好的推广能力。

综上可见，统计学习理论为支持向量机和核函数机器提供了理论基础，也为研究小样本训练时的推广能力提供了一个框架。

然而，该理论也存在局限性：
+ 对大部分常见非线性机器学习模型，对应的函数集和VC维难以估计，导致很多定量的结论难以直接用于指导其他机器学习模型和算法的设计。
+ 深度学习中，很多场景下面临的训练数据虽然有限，但已经超出了统计学习理论所针对的小样本情形，导致VC维基础上得到的各种界都比较松弛。

## 不适定问题和正则化方法简介
### 不适定问题
有些研究者把机器学习问题抽象为一个求解**反演问题**的任务来进行研究：例如，假设研究对象具有某种我们感兴趣但不易观测的特性$\bm{z}$，但可以观测到它经过一定映射后的另外的特性$\bm{u}$，例如最简单情况下它和$\bm{z}$有如下的关系：
$$A\bm{z} = \bm{u}$$

由于其中的映射算子$A$并不知道，我们的机器学习就是用一系列对$\bm{z}$和$\bm{u}$的观测作为样本进行训练，试图获得$A$的逆算子$A^{-1}$，以便能够对未来新的观测$\bm{u}$计算出$\bm{z}$。

然而，即使$A^{-1}$存在且唯一，该问题也不一定是适定的：我们得到的观测$\bm{u}$通常是带有噪声的某种近似$\tilde{\bm{u}}$，而逆算子$A^{-1}$常常是不连续的，导致带有噪声的观测$\tilde{\bm{u}}$有微小变化时，$\bm{z} = A^{-1}\tilde{\bm{u}}$可能会有很大变化。此时，该问题就是**不适定问题**。

在数学上，于很多情况下求解算子方程$$Af = F, \quad f \in \mathscr{F}$$的问题是不适定的；即，即使方程存在唯一解，方程右边的微小扰动$\parallel F - F_{\delta}\parallel < \delta$会带来解的很大变化。这种情况下，在无法得到准确的观测$F$的情况下，对带有噪声的观测$F_{\delta}$用常见的最小化下面的目标泛函$$R(f) = \parallel Af - F_{\delta} \parallel^2$$的方法无法得到对解$f$的好的估计，即使扰动$\delta \to 0$也如此。

### 正则化方法
对于不适定问题，不能使用$R(f) = \parallel Af - F_{\delta} \parallel^2$来求解，而应该使用如下的**正则化泛函**：
$$R^*(f) = \parallel Af - F_{\delta} \parallel^2 + \lambda(\delta) \varOmega(f)$$

其中$\varOmega(f)$为度量解$f$的某种性质的泛函，$\lambda(\delta)$是与观测水平相关的、需要适当选取的常数。
对照统计学习理论中主要结论，可以看到$R(f) = \parallel Af - F_{\delta} \parallel^2$的目标近似于经验风险最小化，而$R^*(f) = \parallel Af - F_{\delta} \parallel^2 + \lambda(\delta) \varOmega(f)$的目标则近似于结构风险最小化。在结构风险最小化的式子$R(\alpha) \leq R_{\text{emp}}(\alpha) + \varPhi(\dfrac{h}{l})$中，$\varPhi(\dfrac{h}{l})$可以看做是对解函数的某种正则化目标。

对于支持向量机，其正则化框架表述如下：
设待求函数为$f(x) = h(x) + b$，其中$h$为由核函数$K$确定的、可再生希尔伯特空间$H_K$中的函数，待求的分类器是对$f(x)$取符号，即$\varPhi(x) = \text{sgn}(f(x))$，支持向量机就是在$l$个训练样本对$(x_i, y_i),\ i = 1, 2, ..., l$下最小化下述目标函数
$$\min_f \dfrac{1}{l}\sum_{i = 1}^l(1 - y_i f(x_i))_+ + \lambda \parallel h \parallel^2_{H_K}$$

此处目标函数的第一项对应着支持向量机原问题中用松弛因子表示的分类错误惩罚，第二项对应着支持向量机原问题中最大化间隔的项，两项之间的折中系数变成了这里的正则化系数$\lambda$。在此基础上发展出了多分类支持向量机。

### 常见的正则化方法
用$\beta$表示回归函数中的参数向量，$V(y_j,\beta^T x_j)$表示回归误差的某种度量。
+ **$L_0$正则化：** $$\min_{\beta} \dfrac{1}{l} \sum_{i = 1}^l V(y_j, \beta^T x_j) + \lambda \parallel \beta \parallel_0$$其中$\parallel \beta \parallel_0$表示向量$\beta$中非零参数个数。
+ **$L_1$正则化：**（Lasso或基追踪算法） $$\min_{\beta} \dfrac{1}{l} \sum_{i = 1}^l (y_j - \beta^T x_j)^2 + \lambda \parallel \beta \parallel_1$$其中$\parallel \beta \parallel_1$表示向量$\beta$中所有参数绝对值之和。
+ **$L_2$正则化：**（Tikhonov正则化） $$\min_{\beta} \dfrac{1}{l} \sum_{i = 1}^l V(y_j, \beta^T x_j) + \lambda \parallel \beta \parallel^2$$
+ **$L_q$正则化：** $$\min_{\beta} \dfrac{1}{l} \sum_{i = 1}^l V(y_j, \beta^T x_j) + \lambda \sum_j|\beta_j^q|^{\frac{1}{q}}$$
+ **弹性网（混合正则化）：** $$\min_{\beta} \dfrac{1}{l} \sum_{i = 1}^l (y_j - \beta^T x_j)^2 + \lambda (\alpha \parallel \beta \parallel_1 + (1 - \alpha)\parallel \beta \parallel^2)$$

常见的如$L_0$正则化在要求经验风险最小化的同时希望函数中非零参数的个数尽量少，实现在减小训练误差的同时实现特征选择的功能，也就是常说的学习对样本特征的稀疏表示。然而，$L_0$范数的优化很难，而$L_1$也可以用来作为对非零参数个数的一种惩罚且优化较容易，从而比较广泛地被使用。
$L_2$范数由于采用了平方和，能够有效地防止参数变得过大，避免过拟合，但对于强制小参数变为$0$的作用不大。
